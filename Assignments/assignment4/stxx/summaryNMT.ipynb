{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed53866a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shaozhetao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import batch_iter\n",
    "from vocab import Vocab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf452e",
   "metadata": {},
   "source": [
    "helper fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870ca2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(file_path, source):\n",
    "    # Understood\n",
    "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
    "    @param file_path (str): path to file containing corpus\n",
    "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
    "        is of the source language or target language\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for line in open(file_path):\n",
    "        sent = nltk.word_tokenize(line)\n",
    "        # only append <s> and </s> to the target sentence\n",
    "        if source == 'tgt':\n",
    "            sent = ['<s>'] + sent + ['</s>']\n",
    "        data.append(sent)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405bd32",
   "metadata": {},
   "source": [
    "define config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca7508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "EMBED_SIZE = 4\n",
    "HIDDEN_SIZE = 3\n",
    "DROPOUT_RATE = 0.0\n",
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404ca627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data & vocabulary\n",
    "train_data_src = read_corpus('./sanity_check_en_es_data/train_sanity_check.es', 'src')\n",
    "train_data_tgt = read_corpus('./sanity_check_en_es_data/train_sanity_check.en', 'tgt')\n",
    "train_data = list(zip(train_data_src, train_data_tgt))\n",
    "\n",
    "for src_sents, tgt_sents in batch_iter(train_data, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    src_sents = src_sents\n",
    "    tgt_sents = tgt_sents\n",
    "    break\n",
    "vocab = Vocab.load('./sanity_check_en_es_data/vocab_sanity_check.json') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767ffe5",
   "metadata": {},
   "source": [
    "### Section 0 - Context "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af70a41",
   "metadata": {},
   "source": [
    "1. understanding vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43587a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocab.VocabEntry"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.src.word2id \n",
    "# 1. shows dict(key=word, val=id) # rank <pad>:0, <s>:1, </s>:2\n",
    "# 2. vocab.src/tgt  is VocabEntry object\n",
    "type(vocab.tgt )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef21549",
   "metadata": {},
   "source": [
    "2. Define backbone Embedding and NMT - assume nothing added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa2d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Set, Union\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, vocab):\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # default values\n",
    "        self.source = None\n",
    "        self.target = None\n",
    "\n",
    "        src_pad_token_idx = vocab.src['<pad>']\n",
    "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
    "\n",
    "        self.source = nn.Embedding(len(vocab.src), embed_size, src_pad_token_idx)\n",
    "        self.target = nn.Embedding(len(vocab.tgt), embed_size, tgt_pad_token_idx)\n",
    "\n",
    "class NMT(nn.Module):\n",
    "    \"\"\" Simple Neural Machine Translation Model:\n",
    "        - Bidrectional LSTM Encoder\n",
    "        - Unidirection LSTM Decoder\n",
    "        - Global Attention Model (Luong, et al. 2015)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
    "        super(NMT, self).__init__()\n",
    "        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # default values\n",
    "#         self.encoder = None \n",
    "#         self.decoder = None\n",
    "#         self.h_projection = None\n",
    "#         self.c_projection = None\n",
    "#         self.att_projection = None\n",
    "#         self.combined_output_projection = None\n",
    "#         self.target_vocab_projection = None\n",
    "#         self.dropout = None\n",
    "        # For sanity check only, not relevant to implementation\n",
    "        self.gen_sanity_check = False\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.hidden_size, bidirectional=True, bias=True)\n",
    "        self.decoder = nn.LSTMCell(input_size=embed_size+self.hidden_size, hidden_size=self.hidden_size, bias=True)\n",
    "\n",
    "        self.h_projection = nn.Linear(self.hidden_size*2, self.hidden_size, bias=False)\n",
    "        self.c_projection = nn.Linear(self.hidden_size*2, self.hidden_size, bias=False)\n",
    "\n",
    "        self.att_projection = nn.Linear(self.hidden_size*2, self.hidden_size, bias=False)\n",
    "        self.combined_output_projection = nn.Linear(self.hidden_size*3, self.hidden_size, bias=False)\n",
    "        self.target_vocab_projection = nn.Linear(self.hidden_size, len(self.vocab.tgt), bias=False)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7ff2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMT(\n",
    "        embed_size=EMBED_SIZE,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        dropout_rate=DROPOUT_RATE,\n",
    "        vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2a8f3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMT(\n",
       "  (model_embeddings): ModelEmbeddings(\n",
       "    (source): Embedding(77, 4, padding_idx=0)\n",
       "    (target): Embedding(85, 4, padding_idx=0)\n",
       "  )\n",
       "  (encoder): LSTM(4, 3, bidirectional=True)\n",
       "  (decoder): LSTMCell(7, 3)\n",
       "  (h_projection): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (c_projection): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (att_projection): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (combined_output_projection): Linear(in_features=9, out_features=3, bias=False)\n",
       "  (target_vocab_projection): Linear(in_features=3, out_features=85, bias=False)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26a554",
   "metadata": {},
   "source": [
    "3. The data we work on testing src_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01def0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordids = [[vocab.src[w] for w in s] for s in src_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e075a96",
   "metadata": {},
   "source": [
    "### Section1: Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75eb50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7165a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lengths = [len(s) for s in src_sents]\n",
    "source_padded = model.vocab.src.to_input_tensor(src_sents, device='cpu') # (src_len, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28eaf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_padded.shape  # torch.Size([22, 5]) dim x batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31b73b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embeddings = ModelEmbeddings(EMBED_SIZE, vocab)\n",
    "X = model_embeddings.source(source_padded) #(src_len, b, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "770a89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xz = pack_padded_sequence(X, torch.tensor(source_lengths)) #packed (src_len, b, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef968abc",
   "metadata": {},
   "source": [
    "Define encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9015ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.LSTM(input_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, bidirectional=True, bias=True)\n",
    "enc_hiddens_pack, (last_hidden, last_cell) = encoder(Xz) #(src_len, b, h*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99b6ed73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(4, 3, bidirectional=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder # input size is embedding size 4, hidden size is hidden size???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bfea4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden.shape # (Bidirectional * b, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29c7a032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_cell.shape #(Bidirectional * b, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b0ea109",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_hiddens, source_lengths = pad_packed_sequence(enc_hiddens_pack, batch_first=True)\n",
    "\n",
    "# source_lengths: tensor([22, 14, 10, 10,  6]), defined earlier in embedding\n",
    "# enc_hiddens.shape, (b, src_len, h*2) e.g. torch.Size([5, 22, 6]) 这里的src_len可以想象成每一步 rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc55c6",
   "metadata": {},
   "source": [
    "### Section 2 decoder+attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca243737",
   "metadata": {},
   "source": [
    "target prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72034d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_padded = vocab.tgt.to_input_tensor(tgt_sents, device='cpu') #torch.Size([24, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e18a41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_padded = target_padded[:-1] # torch.Size([23, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a09e0e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = model_embeddings.target(target_padded) #torch.Size([23, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4d0acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ts = torch.split(Y, 1) #spliting into a tuple of 23 elements, each is tensor of [5,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a547125",
   "metadata": {},
   "source": [
    "#### section 2.1 decode step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4e68ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = HIDDEN_SIZE\n",
    "h_projection = nn.Linear(hidden_size*2, hidden_size, bias=False)\n",
    "c_projection = nn.Linear(hidden_size*2, hidden_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbcda063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d966a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_decoder_hidden/cell: first concate bidirectional hidden together to get h*2, and then a linear layer to h\n",
    "init_decoder_hidden = h_projection(torch.cat((last_hidden[0], last_hidden[1]),1))  #(b, h)\n",
    "init_decoder_cell = c_projection(torch.cat((last_cell[0], last_cell[1]), 1)) #(b, h)\n",
    "\n",
    "dec_init_state = (init_decoder_hidden, init_decoder_cell) # a tuple of 2 (b,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54f51f",
   "metadata": {},
   "source": [
    "#### Section 2.2 now attention, in the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4891c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_projection = nn.Linear(hidden_size*2, hidden_size, bias=False)\n",
    "enc_hiddens_proj = att_projection(enc_hiddens) #(b, src_len, h*2) -> (b, src_len, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97576b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_prev = torch.zeros(BATCH_SIZE, HIDDEN_SIZE, device='cpu') #=(b, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5476678",
   "metadata": {},
   "source": [
    "**note of one step**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df7ebb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "combined_output_projection = nn.Linear(hidden_size*3, hidden_size, bias=False)\n",
    "dropout = nn.Dropout(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28720148",
   "metadata": {},
   "source": [
    "example of one step in attention (not in the final, only show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4bedb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_state = dec_init_state\n",
    "dec_hidden = dec_state[0] #(b,e)\n",
    "Y_t = Y_ts[0]\n",
    "Y_t = torch.squeeze(Y_t)  # torch.Size([1, 5, 4]) -> torch.Size([5, 4]), (b,e) target embedding\n",
    "Ybar_t = torch.cat((Y_t, o_prev), 1) # (b, e+h)\n",
    "# enc_hiddens_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32e1fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsqueeze_dec_hidden = dec_hidden.unsqueeze(2) # torch.Size([5, 3, 1]) (b, h) -> (b, h, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "709f53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_t = torch.bmm(enc_hiddens_proj, unsqueeze_dec_hidden).squeeze(2) #    enc_hiddens_proj=(b, src_len, h)\n",
    "# enc_hiddens_proj=(b, src_len, h) -> e_t =(b, src_len, 1) -> e_t =(b, src_len)\n",
    "# all encoding steps are saved in src_len of enc_hiddens_proj\n",
    "# here key = enc_hiddens, query is specific dec_hidden, in this example is dec_init_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd8c2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_t = F.softmax(e_t, dim=1) # (b, src_lens): This shows attention to all encoding layer\n",
    "\n",
    "a_t = torch.bmm(alpha_t.unsqueeze(1), enc_hiddens).squeeze(1) #(b, h*2): This provides attention results \n",
    "# Here value set is enc_hiddens\n",
    "\n",
    "u_t = torch.cat((a_t, dec_hidden), dim=1) #(b, h*3)  #concat attention with hidden and pass to next state\n",
    "v_t = combined_output_projection(u_t) #(b, h) #\n",
    "O_t = dropout(torch.tanh(v_t)) #(b, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d7844",
   "metadata": {},
   "source": [
    "**note of one step compelte**\n",
    "\n",
    "Complete one step example of attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf47c27",
   "metadata": {},
   "source": [
    "Let's put things together! Following is for all loop of decoder step (each loop walks through all encoder for attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d5a01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define decoder \n",
    "decoder = nn.LSTMCell(input_size=EMBED_SIZE+HIDDEN_SIZE, hidden_size=HIDDEN_SIZE, bias=True)\n",
    "# 不直接用LSTM作为decoder是因为， 每个decoder出来的(dec_hidden, dec_cell)将被用来求attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea12dfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1309, -0.0195, -0.1640],\n",
       "        [-0.1616, -0.0361, -0.1463],\n",
       "        [ 0.1730, -0.0910, -0.3136],\n",
       "        [-0.1822, -0.0143, -0.0988],\n",
       "        [ 0.2522, -0.0716, -0.3942]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c545035",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a mask\n",
    "enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n",
    "for e_id, src_len in enumerate(source_lengths):\n",
    "    enc_masks[e_id, src_len:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f9953f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# already defined o_prev, dec_state, Y, Y_ts, enc_hidden_proj\n",
    "combined_outputs = []\n",
    "for i in range(len(Y_ts)):\n",
    "    Y_t = Y_ts[i]\n",
    "    Y_t = torch.squeeze(Y_t)\n",
    "    Ybar_t = torch.cat((Y_t, o_prev), 1)\n",
    "    # one loop\n",
    "    dec_hidden = dec_state[0]\n",
    "    unsqueeze_dec_hidden = dec_hidden.unsqueeze(2) \n",
    "    e_t = torch.bmm(enc_hiddens_proj, unsqueeze_dec_hidden).squeeze(2)\n",
    "    # 这里可能要一个masks\n",
    "    if enc_masks is not None:\n",
    "        e_t.data.masked_fill_(enc_masks.bool(), -float('inf'))\n",
    "    alpha_t = F.softmax(e_t, dim=1) \n",
    "    a_t = torch.bmm(alpha_t.unsqueeze(1), enc_hiddens).squeeze(1)\n",
    "    u_t = torch.cat((a_t, dec_hidden), dim=1)\n",
    "    v_t = combined_output_projection(u_t)\n",
    "    o_t = dropout(torch.tanh(v_t))\n",
    "    o_prev = o_t\n",
    "    # one loop complete\n",
    "    dec_state = decoder(Ybar_t, dec_state) # refresh dec_state\n",
    "    combined_outputs.append(o_t)\n",
    "combined_outputs = torch.stack(combined_outputs, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6d1ba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be28c19c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0898,  0.0389,  0.0026],\n",
       "        [-0.1102,  0.0464,  0.0089],\n",
       "        [-0.0738,  0.0496,  0.0310],\n",
       "        [-0.0970,  0.0252,  0.0082],\n",
       "        [-0.0419,  0.0445,  0.0305]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c1270",
   "metadata": {},
   "source": [
    "#### Section 2.X validation - no way to validate due to embed_size and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outputs_target = torch.load('./sanity_check_en_es_data/combined_outputs.pkl')\n",
    "print(combined_outputs_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040542e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_outputs_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06053314",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_init_state = torch.load('./sanity_check_en_es_data/dec_init_state.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa46b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_init_state[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb705b",
   "metadata": {},
   "source": [
    "### Section 3: Loss and forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0805efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_projection = nn.Linear(HIDDEN_SIZE, len(vocab.tgt), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4be330",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = F.log_softmax(target_vocab_projection(combined_outputs), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_masks = (target_padded != vocab.tgt['<pad>']).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = target_gold_words_log_prob.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0ea11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlds",
   "language": "python",
   "name": "mlds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
